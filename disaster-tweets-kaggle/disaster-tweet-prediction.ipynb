{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Disaster-Related Tweets\n",
    "\n",
    "This notebook walks through my approach to the Kaggle competition [\"Real or Not? NLP with Disaster Tweets\"](https://www.kaggle.com/competitions/nlp-getting-started). The goal is to build models that classify whether a tweet refers to a real disaster or not. I explore a range of traditional and neural approaches, documenting the reasoning behind each step so the workflow is easy to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-21T19:00:03.228650Z",
     "iopub.status.busy": "2025-10-21T19:00:03.228064Z",
     "iopub.status.idle": "2025-10-21T19:00:06.264693Z",
     "shell.execute_reply": "2025-10-21T19:00:06.262820Z",
     "shell.execute_reply.started": "2025-10-21T19:00:03.228620Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/4192049545.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# libraries import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m )\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplotting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtseries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/api/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\" public toolkit API \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m from pandas.api import (\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mindexers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minterchange\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/api/typing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# from pandas.io.formats.style import Styler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJsonReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStataReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m __all__ = [\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# libraries import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize, ToktokTokenizer\n",
    "import nltk\n",
    "\n",
    "import spacy\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T20:18:26.057706Z",
     "iopub.status.busy": "2025-10-07T20:18:26.057187Z",
     "iopub.status.idle": "2025-10-07T20:18:58.076777Z",
     "shell.execute_reply": "2025-10-07T20:18:58.075917Z",
     "shell.execute_reply.started": "2025-10-07T20:18:26.057682Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T20:18:58.077868Z",
     "iopub.status.busy": "2025-10-07T20:18:58.077581Z",
     "iopub.status.idle": "2025-10-07T20:18:58.130681Z",
     "shell.execute_reply": "2025-10-07T20:18:58.129907Z",
     "shell.execute_reply.started": "2025-10-07T20:18:58.077845Z"
    }
   },
   "outputs": [],
   "source": [
    "# data load\n",
    "train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
    "sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "\n",
    "The competition provides three CSV files: a training set with labels, a test set without labels, and a sample submission. Each tweet comes with an `id`, free-text message, keyword metadata, and location information. The cells below load the data and display random samples so we can sanity-check the structure and look for quirks such as missing keywords or location fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split train_df into train and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T20:18:58.132577Z",
     "iopub.status.busy": "2025-10-07T20:18:58.132332Z",
     "iopub.status.idle": "2025-10-07T20:18:58.140747Z",
     "shell.execute_reply": "2025-10-07T20:18:58.139783Z",
     "shell.execute_reply.started": "2025-10-07T20:18:58.132552Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_df['text'], train_df['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model: CountVectorizer\n",
    "\n",
    "As a starting point, I build a simple bag-of-words model using `CountVectorizer` to transform text into term counts and train a logistic regression classifier. This baseline establishes a performance floor and helps confirm that the preprocessing pipeline works end-to-end before introducing more sophisticated representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1))\n",
    "bow = vec.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(vec.transform(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Features\n",
    "\n",
    "Next, I replace raw counts with TF-IDF weights, which down-weight ubiquitous words and emphasize more informative terms. Keeping the same logistic regression classifier allows for a direct comparison with the count-based baseline to see how much the richer weighting scheme improves the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1, 1))\n",
    "bow = vec.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(vec.transform(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Lemmatization\n",
    "\n",
    "To capture more linguistic nuance, I add a custom preprocessing function that tokenizes text, removes stop words and punctuation, and lemmatizes tokens. Feeding these cleaned tokens into a TF-IDF vectorizer often reduces noise from inflected forms (e.g., \"run\" vs. \"running\") and can yield a more generalizable representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_tok = ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = stopwords.words('english') + list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_preproc(text):\n",
    "    text = re.sub(f\"[{punctuation}]\", \"\", text)\n",
    "    doc = nlp(text)\n",
    "    return [\n",
    "        token.lemma_ \n",
    "        for token in doc \n",
    "        if not token.is_punct \n",
    "        and token.lemma_ not in noise \n",
    "        and token.lemma_.strip() != \"\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_preproc('The dogs are running fast.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=my_preproc)\n",
    "bow = vec.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(vec.transform(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(vec.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.concat([test_df['id'], pd.Series(y_pred)], axis=1)\n",
    "output.columns = ['id', 'target']\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings: Word2Vec\n",
    "\n",
    "Bag-of-words models ignore semantic relationships between words. To address that, I train a Word2Vec model on the competition tweets and average the resulting word vectors for each message. The averaged embeddings serve as dense features for logistic regression, enabling the classifier to leverage similarities between related words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Skip-Gram Model\n",
    "\n",
    "This section prepares the corpus, trains a skip-gram Word2Vec model with negative sampling, and converts tweets into fixed-size vectors. Training on the competition data keeps the vocabulary aligned with the informal language found on Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # remove URLs\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # keep only letters\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "X_train_tokens = [preprocess(text) for text in X_train]\n",
    "X_valid_tokens = [preprocess(text) for text in X_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    sentences=X_train_tokens,\n",
    "    vector_size=100,     # you can try 200 or 300\n",
    "    window=5,            # context window\n",
    "    min_count=2,         # ignore rare words\n",
    "    workers=4,\n",
    "    sg=1                 # 1 for skip-gram, 0 for CBOW\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(\"word2vec_tweets.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_vec(tokens, model, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        if word in model.wv:\n",
    "            vec += model.wv[word].reshape((1, size))\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "SIZE = 100\n",
    "X_train_vecs = np.concatenate([tweet_to_vec(tokens, w2v_model, SIZE) for tokens in X_train_tokens])\n",
    "X_valid_vecs = np.concatenate([tweet_to_vec(tokens, w2v_model, SIZE) for tokens in X_valid_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_vecs, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_valid_vecs)\n",
    "print(\"F1-score:\", f1_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pretrained GloVe Embeddings\n",
    "\n",
    "I also experiment with pretrained 300-dimensional GloVe vectors learned on a massive Twitter corpus. Instead of training embeddings from scratch, I map each token to its pretrained vector and average them. This approach often improves generalization because the embeddings already encode broader world knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(\"/kaggle/input/glove-6b-300d/glove.6B.300d.txt\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = vector\n",
    "\n",
    "print(\"Loaded %d word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_vec(tokens, embeddings, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        if word in embeddings:\n",
    "            vec += embeddings[word].reshape((1, size))\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "\n",
    "size = 300  # because we're using GloVe 100d\n",
    "\n",
    "X_train_vecs = np.concatenate([tweet_to_vec(tokens, embeddings_index, size) for tokens in X_train_tokens])\n",
    "X_valid_vecs = np.concatenate([tweet_to_vec(tokens, embeddings_index, size) for tokens in X_valid_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_vecs, y_train)\n",
    "y_pred = clf.predict(X_valid_vecs)\n",
    "print(\"F1-score:\", f1_score(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Models: CNN\n",
    "\n",
    "Finally, I explore a convolutional neural network that learns embeddings jointly with the classifier. After tokenizing and padding the tweets, the CNN stacks embedding, convolutional, and pooling layers before a dense classifier. This architecture can capture local n-gram patterns and sometimes outperform linear models on short-text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Настроим токенайзер\n",
    "max_words = 20000   # словарь: топ-20k самых частых слов\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# 2. Превратим тексты в последовательности чисел\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_valid_seq = tokenizer.texts_to_sequences(X_valid)\n",
    "\n",
    "# 3. Паддинг до одинаковой длины\n",
    "max_len = 50  # можно подобрать по распределению длин\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_valid_pad = pad_sequences(X_valid_seq, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100  # размерность эмбеддингов\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # бинарная классификация\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid_pad, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказания\n",
    "y_pred_prob = model.predict(X_valid_pad)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# F1-score\n",
    "f1 = f1_score(y_valid, y_pred)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "probs = model.predict(X_valid_pad).ravel()\n",
    "ths = np.linspace(0.1, 0.9, 81)\n",
    "best = max(((t, f1_score(y_valid, (probs>=t).astype(int))) for t in ths), key=lambda x: x[1])\n",
    "best_threshold, best_f1 = best\n",
    "print(best_threshold, best_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # лемматизация\n",
    "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text_clean'] = train_df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df['text_clean'],\n",
    "    train_df['target'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# 1️⃣ Импорты\n",
    "# ========================================\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# ========================================\n",
    "# 2️⃣ Предобработка текста (создаём копию!)\n",
    "# ========================================\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)            # удаляем ссылки\n",
    "    text = re.sub(r'@\\w+', '', text)                      # убираем упоминания\n",
    "    text = re.sub(r'#\\w+', '', text)                      # убираем хэштеги\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # пунктуация\n",
    "    text = re.sub(r'\\d+', '', text)                       # цифры\n",
    "    text = text.lower().strip()                          # нижний регистр\n",
    "    text = re.sub(r'\\s+', ' ', text)                     # лишние пробелы\n",
    "    return text\n",
    "\n",
    "train_df['text_clean'] = train_df['text'].apply(clean_text)\n",
    "\n",
    "# ========================================\n",
    "# 3️⃣ train/valid split\n",
    "# ========================================\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df['text_clean'],\n",
    "    train_df['target'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ========================================\n",
    "# 4️⃣ Токенизация и паддинг\n",
    "# ========================================\n",
    "max_words = 20000\n",
    "max_len = 50\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_valid_seq = tokenizer.texts_to_sequences(X_valid)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_valid_pad = pad_sequences(X_valid_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# ========================================\n",
    "# 5️⃣ Архитектура TextCNN\n",
    "# ========================================\n",
    "embedding_dim = 100\n",
    "\n",
    "inp = Input(shape=(max_len,))\n",
    "emb = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len)(inp)\n",
    "\n",
    "# три свёртки с разными окнами\n",
    "conv_blocks = []\n",
    "for kernel_size in [3, 4, 5]:\n",
    "    conv = Conv1D(filters=128, kernel_size=kernel_size, activation='relu', padding='valid')(emb)\n",
    "    pool = GlobalMaxPooling1D()(conv)\n",
    "    conv_blocks.append(pool)\n",
    "\n",
    "x = Concatenate()(conv_blocks)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inp, out)\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ========================================\n",
    "# 6️⃣ Колбэки (ранняя остановка + LR scheduler)\n",
    "# ========================================\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=1e-5)\n",
    "]\n",
    "\n",
    "# ========================================\n",
    "# 7️⃣ Обучение модели\n",
    "# ========================================\n",
    "history = model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    validation_data=(X_valid_pad, y_valid),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ========================================\n",
    "# 8️⃣ Предсказания и подбор оптимального порога\n",
    "# ========================================\n",
    "y_prob = model.predict(X_valid_pad).ravel()\n",
    "\n",
    "best_f1, best_thr = 0, 0\n",
    "for thr in np.linspace(0.1, 0.9, 81):\n",
    "    f1 = f1_score(y_valid, (y_prob >= thr).astype(int))\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_thr = f1, thr\n",
    "\n",
    "print(f\"✅ Лучший порог: {best_thr:.2f}\")\n",
    "print(f\"✅ Лучший F1-score: {best_f1:.4f}\")\n",
    "\n",
    "# ========================================\n",
    "# 9️⃣ Финальная метрика\n",
    "# ========================================\n",
    "y_pred = (y_prob >= best_thr).astype(int)\n",
    "print(\"F1-score (final):\", f1_score(y_valid, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T20:18:58.142206Z",
     "iopub.status.busy": "2025-10-07T20:18:58.141887Z",
     "iopub.status.idle": "2025-10-07T20:22:28.572725Z",
     "shell.execute_reply": "2025-10-07T20:22:28.571588Z",
     "shell.execute_reply.started": "2025-10-07T20:18:58.142174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78a08f06a150>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/transformers/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78a08ef37790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/transformers/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78a08ef43650>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/transformers/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78a08f1d1810>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/transformers/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78a08f0a3390>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/transformers/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement transformers==4.44.2 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for transformers==4.44.2\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:22:21.357679: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759868541.391606     741 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759868541.401945     741 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 0) Установка библиотек (в Kaggle ноутбуке раскомментируй)\n",
    "# =========================================================\n",
    "!pip -q install transformers==4.44.2 datasets==3.0.1 accelerate==0.34.2\n",
    "\n",
    "import re, os, random, math, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, Trainer, TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T20:22:28.574773Z",
     "iopub.status.busy": "2025-10-07T20:22:28.574048Z",
     "iopub.status.idle": "2025-10-07T20:22:32.016996Z",
     "shell.execute_reply": "2025-10-07T20:22:32.015699Z",
     "shell.execute_reply.started": "2025-10-07T20:22:28.574743Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_741/841732712.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0mcollator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataCollatorWithPadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m args = TrainingArguments(\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./checkpoints\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mevaluation_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "set_seed()\n",
    "\n",
    "# =========================================================\n",
    "# 2) Мягкая очистка твитов (не трогаем оригинал)\n",
    "#    Важно: ссылки/упоминания -> специальные токены (не удаляем!)\n",
    "# =========================================================\n",
    "URL_TOKEN  = \"__URL__\"\n",
    "USER_TOKEN = \"__USER__\"\n",
    "\n",
    "def clean_tweet(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Превращаем ссылки/упоминания в токены\n",
    "    text = re.sub(r'http\\S+|www\\S+', URL_TOKEN, text)\n",
    "    text = re.sub(r'@\\w+', USER_TOKEN, text)\n",
    "    # Оставляем хэштеги как слова без '#'\n",
    "    text = re.sub(r'#', '', text)\n",
    "    # Нормализуем пробелы, нижний регистр\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "train_df[\"text_clean\"] = train_df[\"text\"].apply(clean_tweet)\n",
    "test_df[\"text_clean\"]  = test_df[\"text\"].apply(clean_tweet)\n",
    "\n",
    "# =========================================================\n",
    "# 3) Сплит (если свой уже есть — можно использовать его)\n",
    "# =========================================================\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train_df[\"text_clean\"], train_df[\"target\"],\n",
    "    test_size=0.15, random_state=SEED, stratify=train_df[\"target\"]\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# 4) Токенизация под твиты (BERTweet)\n",
    "# =========================================================\n",
    "MODEL_PATH = \"/kaggle/input/bertweet/bertweet-local\"   # альтернатива: \"roberta-base\" / \"microsoft/deberta-v3-base\"\n",
    "\n",
    "# У BERTweet есть нормализатор эмодзи/твит-специфики в токенизаторе\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=True)\n",
    "\n",
    "MAX_LEN = 96  # 80–112 обычно оптимально для твитов\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        padding=False,              # паддинг сделает collator\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "train_enc = tokenize_texts(X_train)\n",
    "valid_enc = tokenize_texts(X_valid)\n",
    "test_enc  = tokenize_texts(test_df[\"text_clean\"])\n",
    "\n",
    "# =========================================================\n",
    "# 5) Torch Dataset\n",
    "# =========================================================\n",
    "class TxtDataset(Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k,v in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            item[\"labels\"] = torch.tensor(int(self.labels.iloc[idx]))\n",
    "        return item\n",
    "\n",
    "ds_train = TxtDataset(train_enc, y_train.reset_index(drop=True))\n",
    "ds_valid = TxtDataset(valid_enc, y_valid.reset_index(drop=True))\n",
    "ds_test  = TxtDataset(test_enc,  None)\n",
    "\n",
    "# =========================================================\n",
    "# 6) Модель\n",
    "# =========================================================\n",
    "num_labels = 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# 7) Метрики: F1 по валидации (дальше подберём порог)\n",
    "# =========================================================\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()\n",
    "    preds = (probs >= 0.5).astype(int)  # временно 0.5\n",
    "    return {\"f1\": f1_score(labels, preds)}\n",
    "\n",
    "# =========================================================\n",
    "# 8) Тренировка\n",
    "# =========================================================\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 4\n",
    "LR = 2e-5\n",
    "\n",
    "# Data collator сам паддит до батча\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    warmup_ratio=0.06,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[]  # отключим wandb/tensorboard по умолчанию\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_valid,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# =========================================================\n",
    "# 9) Подбор Порога по F1 на валидации\n",
    "# =========================================================\n",
    "valid_logits = trainer.predict(ds_valid).predictions\n",
    "valid_probs  = torch.softmax(torch.tensor(valid_logits), dim=1)[:, 1].numpy()\n",
    "\n",
    "best_thr, best_f1 = 0.5, 0.0\n",
    "for thr in np.linspace(0.1, 0.9, 161):\n",
    "    f1 = f1_score(y_valid.values, (valid_probs >= thr).astype(int))\n",
    "    if f1 > best_f1:\n",
    "        best_f1, best_thr = f1, thr\n",
    "\n",
    "print(f\"Best F1={best_f1:.4f} @ threshold={best_thr:.3f}\")\n",
    "\n",
    "print(classification_report(\n",
    "    y_valid.values, (valid_probs >= best_thr).astype(int), digits=4)\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# 10) Обучение на полном train (опционально для сабмита)\n",
    "#     Склеим train+valid, дообучим 1 эпоху c маленьким LR\n",
    "# =========================================================\n",
    "FULL_FINETUNE = True\n",
    "if FULL_FINETUNE:\n",
    "    X_full = pd.concat([X_train, X_valid], ignore_index=True)\n",
    "    y_full = pd.concat([y_train, y_valid], ignore_index=True)\n",
    "\n",
    "    full_enc = tokenize_texts(X_full)\n",
    "    ds_full  = TxtDataset(full_enc, y_full)\n",
    "\n",
    "    # уменьшим LR и эпоху — модель уже натренирована\n",
    "    args_full = TrainingArguments(\n",
    "        output_dir=\"./checkpoints_full\",\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=1e-5,\n",
    "        warmup_ratio=0.0,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        evaluation_strategy=\"no\",\n",
    "        save_strategy=\"no\",\n",
    "        report_to=[]\n",
    "    )\n",
    "\n",
    "    trainer_full = Trainer(\n",
    "        model=trainer.model,  # берём лучшую модель\n",
    "        args=args_full,\n",
    "        train_dataset=ds_full,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator\n",
    "    )\n",
    "    trainer_full.train()\n",
    "\n",
    "# =========================================================\n",
    "# 11) Предсказания на test и формирование submission.csv\n",
    "# =========================================================\n",
    "test_logits = trainer.model(**collator(ds_test[:])).logits.detach().cpu().numpy() \\\n",
    "    if len(ds_test) < BATCH_SIZE else trainer.predict(ds_test).predictions\n",
    "test_probs  = torch.softmax(torch.tensor(test_logits), dim=1)[:, 1].numpy()\n",
    "test_preds  = (test_probs >= best_thr).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],\n",
    "    \"target\": test_preds\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Saved submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c41dfae",
   "metadata": {},
   "source": [
    "## Conclusions and Next Steps\n",
    "\n",
    "Across these experiments, classical linear models with TF-IDF features provide strong and stable baselines, especially when combined with careful text normalization. Word embedding approaches open the door to capturing semantic similarity, and the CNN offers a foundation for deeper architectures. Future improvements could include hyperparameter optimization, cross-validation, ensembling several models, and fine-tuning transformer-based architectures (e.g., BERT) once a GPU runtime is available."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 869809,
     "sourceId": 17777,
     "sourceType": "competition"
    },
    {
     "datasetId": 8410795,
     "sourceId": 13271911,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8410831,
     "sourceId": 13271964,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8425929,
     "sourceId": 13294083,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
