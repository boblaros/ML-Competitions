{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":13271911,"sourceType":"datasetVersion","datasetId":8410795},{"sourceId":13271964,"sourceType":"datasetVersion","datasetId":8410831},{"sourceId":13294083,"sourceType":"datasetVersion","datasetId":8425929}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Competion Descruption (TBA)","metadata":{}},{"cell_type":"code","source":"# libraries import\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom gensim.models import Word2Vec\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk import ngrams\nfrom nltk.tokenize import word_tokenize, ToktokTokenizer\nimport nltk\n\nimport spacy\n\nfrom sklearn.metrics import f1_score\n\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nimport re","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-21T19:00:03.228064Z","iopub.execute_input":"2025-10-21T19:00:03.228650Z","iopub.status.idle":"2025-10-21T19:00:06.264693Z","shell.execute_reply.started":"2025-10-21T19:00:03.228620Z","shell.execute_reply":"2025-10-21T19:00:06.262820Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/4192049545.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# libraries import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m )\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplotting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtseries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/api/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\" public toolkit API \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m from pandas.api import (\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mindexers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minterchange\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/api/typing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# from pandas.io.formats.style import Styler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJsonReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStataReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m __all__ = [\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"nltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T20:18:26.057187Z","iopub.execute_input":"2025-10-07T20:18:26.057706Z","iopub.status.idle":"2025-10-07T20:18:58.076777Z","shell.execute_reply.started":"2025-10-07T20:18:26.057682Z","shell.execute_reply":"2025-10-07T20:18:58.075917Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n[nltk_data]     failure in name resolution>\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# data load\ntrain_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T20:18:58.077581Z","iopub.execute_input":"2025-10-07T20:18:58.077868Z","iopub.status.idle":"2025-10-07T20:18:58.130681Z","shell.execute_reply.started":"2025-10-07T20:18:58.077845Z","shell.execute_reply":"2025-10-07T20:18:58.129907Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"Let's have a look at the data:","metadata":{}},{"cell_type":"code","source":"train_df.sample(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.sample(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_submission.sample(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's split train_df into train and validation sets:","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(train_df['text'], train_df['target'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T20:18:58.132332Z","iopub.execute_input":"2025-10-07T20:18:58.132577Z","iopub.status.idle":"2025-10-07T20:18:58.140747Z","shell.execute_reply.started":"2025-10-07T20:18:58.132552Z","shell.execute_reply":"2025-10-07T20:18:58.139783Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Basemodel: CountVectorizer","metadata":{}},{"cell_type":"code","source":"vec = CountVectorizer(ngram_range=(1, 1))\nbow = vec.fit_transform(X_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clf = LogisticRegression(random_state=42)\nclf.fit(bow, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = clf.predict(vec.transform(X_valid))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f1_score(y_valid, y_pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## TfidfVectorizer","metadata":{}},{"cell_type":"code","source":"vec = TfidfVectorizer(ngram_range=(1, 1))\nbow = vec.fit_transform(X_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clf = LogisticRegression(random_state=42)\nclf.fit(bow, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = clf.predict(vec.transform(X_valid))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f1_score(y_valid, y_pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tokenization and Lemmatization","metadata":{}},{"cell_type":"code","source":"tt_tok = ToktokTokenizer()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"noise = stopwords.words('english') + list(punctuation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def my_preproc(text):\n    text = re.sub(f\"[{punctuation}]\", \"\", text)\n    doc = nlp(text)\n    return [\n        token.lemma_ \n        for token in doc \n        if not token.is_punct \n        and token.lemma_ not in noise \n        and token.lemma_.strip() != \"\"\n    ]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# my_preproc('The dogs are running fast.')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=my_preproc)\nbow = vec.fit_transform(X_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clf = LogisticRegression(random_state=42)\nclf.fit(bow, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = clf.predict(vec.transform(X_valid))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f1_score(y_valid, y_pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test = test_df['text']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = clf.predict(vec.transform(X_test))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = pd.concat([test_df['id'], pd.Series(y_pred)], axis=1)\noutput.columns = ['id', 'target']\noutput.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# output.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Embeddings: Word2Vec","metadata":{}},{"cell_type":"markdown","source":"### Training a Skip-Gram Model","metadata":{}},{"cell_type":"code","source":"def preprocess(text):\n    text = text.lower()\n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # remove URLs\n    text = re.sub(r'[^a-z\\s]', '', text)  # keep only letters\n    tokens = word_tokenize(text)\n    return tokens\n\nX_train_tokens = [preprocess(text) for text in X_train]\nX_valid_tokens = [preprocess(text) for text in X_valid]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"w2v_model = Word2Vec(\n    sentences=X_train_tokens,\n    vector_size=100,     # you can try 200 or 300\n    window=5,            # context window\n    min_count=2,         # ignore rare words\n    workers=4,\n    sg=1                 # 1 for skip-gram, 0 for CBOW\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"w2v_model.save(\"word2vec_tweets.model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tweet_to_vec(tokens, model, size):\n    vec = np.zeros(size).reshape((1, size))\n    count = 0\n    for word in tokens:\n        if word in model.wv:\n            vec += model.wv[word].reshape((1, size))\n            count += 1\n    if count != 0:\n        vec /= count\n    return vec\n\nSIZE = 100\nX_train_vecs = np.concatenate([tweet_to_vec(tokens, w2v_model, SIZE) for tokens in X_train_tokens])\nX_valid_vecs = np.concatenate([tweet_to_vec(tokens, w2v_model, SIZE) for tokens in X_valid_tokens])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clf = LogisticRegression(max_iter=1000)\nclf.fit(X_train_vecs, y_train)\n\ny_pred = clf.predict(X_valid_vecs)\nprint(\"F1-score:\", f1_score(y_valid, y_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### GloVe","metadata":{}},{"cell_type":"code","source":"embeddings_index = {}\nwith open(\"/kaggle/input/glove-6b-300d/glove.6B.300d.txt\", encoding=\"utf8\") as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vector = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = vector\n\nprint(\"Loaded %d word vectors.\" % len(embeddings_index))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tweet_to_vec(tokens, embeddings, size):\n    vec = np.zeros(size).reshape((1, size))\n    count = 0\n    for word in tokens:\n        if word in embeddings:\n            vec += embeddings[word].reshape((1, size))\n            count += 1\n    if count != 0:\n        vec /= count\n    return vec\n\n\nsize = 300  # because we're using GloVe 100d\n\nX_train_vecs = np.concatenate([tweet_to_vec(tokens, embeddings_index, size) for tokens in X_train_tokens])\nX_valid_vecs = np.concatenate([tweet_to_vec(tokens, embeddings_index, size) for tokens in X_valid_tokens])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clf = LogisticRegression(max_iter=1000)\nclf.fit(X_train_vecs, y_train)\ny_pred = clf.predict(X_valid_vecs)\nprint(\"F1-score:\", f1_score(y_valid, y_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Deep Learning Models: CNN","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Настроим токенайзер\nmax_words = 20000   # словарь: топ-20k самых частых слов\ntokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(X_train)\n\n# 2. Превратим тексты в последовательности чисел\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_valid_seq = tokenizer.texts_to_sequences(X_valid)\n\n# 3. Паддинг до одинаковой длины\nmax_len = 50  # можно подобрать по распределению длин\nX_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\nX_valid_pad = pad_sequences(X_valid_seq, maxlen=max_len, padding='post')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedding_dim = 100  # размерность эмбеддингов\n\nmodel = Sequential([\n    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n    Conv1D(filters=128, kernel_size=3, activation='relu'),\n    GlobalMaxPooling1D(),\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')  # бинарная классификация\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    X_train_pad, y_train,\n    epochs=10,\n    batch_size=32,\n    validation_data=(X_valid_pad, y_valid)\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nimport numpy as np","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Предсказания\ny_pred_prob = model.predict(X_valid_pad)\ny_pred = (y_pred_prob > 0.5).astype(int)\n\n# F1-score\nf1 = f1_score(y_valid, y_pred)\nprint(\"F1-score:\", f1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nimport numpy as np\n\nprobs = model.predict(X_valid_pad).ravel()\nths = np.linspace(0.1, 0.9, 81)\nbest = max(((t, f1_score(y_valid, (probs>=t).astype(int))) for t in ths), key=lambda x: x[1])\nbest_threshold, best_f1 = best\nprint(best_threshold, best_f1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nfrom nltk.stem import WordNetLemmatizer\nimport re\nimport string \nnltk.download('wordnet')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\n\ndef clean_text(text):\n    text = re.sub(r'http\\S+|www\\S+', '', text)\n    text = re.sub(r'@\\w+', '', text)\n    text = re.sub(r'#\\w+', '', text)\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = re.sub(r'\\d+', '', text)\n    text = text.lower()\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    # лемматизация\n    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n    return text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df['text_clean'] = train_df['text'].apply(clean_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(\n    train_df['text_clean'],\n    train_df['target'],\n    test_size=0.2,\n    random_state=42\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ========================================\n# 1️⃣ Импорты\n# ========================================\nimport re\nimport string\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout, Dense\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n# ========================================\n# 2️⃣ Предобработка текста (создаём копию!)\n# ========================================\ndef clean_text(text):\n    text = re.sub(r'http\\S+|www\\S+', '', text)            # удаляем ссылки\n    text = re.sub(r'@\\w+', '', text)                      # убираем упоминания\n    text = re.sub(r'#\\w+', '', text)                      # убираем хэштеги\n    text = text.translate(str.maketrans('', '', string.punctuation))  # пунктуация\n    text = re.sub(r'\\d+', '', text)                       # цифры\n    text = text.lower().strip()                          # нижний регистр\n    text = re.sub(r'\\s+', ' ', text)                     # лишние пробелы\n    return text\n\ntrain_df['text_clean'] = train_df['text'].apply(clean_text)\n\n# ========================================\n# 3️⃣ train/valid split\n# ========================================\nX_train, X_valid, y_train, y_valid = train_test_split(\n    train_df['text_clean'],\n    train_df['target'],\n    test_size=0.2,\n    random_state=42\n)\n\n# ========================================\n# 4️⃣ Токенизация и паддинг\n# ========================================\nmax_words = 20000\nmax_len = 50\n\ntokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(X_train)\n\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_valid_seq = tokenizer.texts_to_sequences(X_valid)\n\nX_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\nX_valid_pad = pad_sequences(X_valid_seq, maxlen=max_len, padding='post')\n\n# ========================================\n# 5️⃣ Архитектура TextCNN\n# ========================================\nembedding_dim = 100\n\ninp = Input(shape=(max_len,))\nemb = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len)(inp)\n\n# три свёртки с разными окнами\nconv_blocks = []\nfor kernel_size in [3, 4, 5]:\n    conv = Conv1D(filters=128, kernel_size=kernel_size, activation='relu', padding='valid')(emb)\n    pool = GlobalMaxPooling1D()(conv)\n    conv_blocks.append(pool)\n\nx = Concatenate()(conv_blocks)\nx = Dropout(0.5)(x)\nx = Dense(64, activation='relu')(x)\nx = Dropout(0.5)(x)\nout = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(inp, out)\n\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)\n\nmodel.summary()\n\n# ========================================\n# 6️⃣ Колбэки (ранняя остановка + LR scheduler)\n# ========================================\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=1e-5)\n]\n\n# ========================================\n# 7️⃣ Обучение модели\n# ========================================\nhistory = model.fit(\n    X_train_pad, y_train,\n    validation_data=(X_valid_pad, y_valid),\n    epochs=10,\n    batch_size=64,\n    callbacks=callbacks,\n    verbose=1\n)\n\n# ========================================\n# 8️⃣ Предсказания и подбор оптимального порога\n# ========================================\ny_prob = model.predict(X_valid_pad).ravel()\n\nbest_f1, best_thr = 0, 0\nfor thr in np.linspace(0.1, 0.9, 81):\n    f1 = f1_score(y_valid, (y_prob >= thr).astype(int))\n    if f1 > best_f1:\n        best_f1, best_thr = f1, thr\n\nprint(f\"✅ Лучший порог: {best_thr:.2f}\")\nprint(f\"✅ Лучший F1-score: {best_f1:.4f}\")\n\n# ========================================\n# 9️⃣ Финальная метрика\n# ========================================\ny_pred = (y_prob >= best_thr).astype(int)\nprint(\"F1-score (final):\", f1_score(y_valid, y_pred))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fine Tuning Bert","metadata":{}},{"cell_type":"code","source":"# =========================================================\n# 0) Установка библиотек (в Kaggle ноутбуке раскомментируй)\n# =========================================================\n!pip -q install transformers==4.44.2 datasets==3.0.1 accelerate==0.34.2\n\nimport re, os, random, math, gc\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import f1_score, classification_report\n\nimport torch\nfrom torch.utils.data import Dataset\n\nfrom transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n                          DataCollatorWithPadding, Trainer, TrainingArguments)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T20:18:58.141887Z","iopub.execute_input":"2025-10-07T20:18:58.142206Z","iopub.status.idle":"2025-10-07T20:22:28.572725Z","shell.execute_reply.started":"2025-10-07T20:18:58.142174Z","shell.execute_reply":"2025-10-07T20:22:28.571588Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78a08f06a150>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/transformers/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78a08ef37790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/transformers/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78a08ef43650>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/transformers/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78a08f1d1810>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/transformers/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78a08f0a3390>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/transformers/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement transformers==4.44.2 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for transformers==4.44.2\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-10-07 20:22:21.357679: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759868541.391606     741 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759868541.401945     741 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"SEED = 42\ndef set_seed(seed=SEED):\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\nset_seed()\n\n# =========================================================\n# 2) Мягкая очистка твитов (не трогаем оригинал)\n#    Важно: ссылки/упоминания -> специальные токены (не удаляем!)\n# =========================================================\nURL_TOKEN  = \"__URL__\"\nUSER_TOKEN = \"__USER__\"\n\ndef clean_tweet(text: str) -> str:\n    if not isinstance(text, str):\n        return \"\"\n    # Превращаем ссылки/упоминания в токены\n    text = re.sub(r'http\\S+|www\\S+', URL_TOKEN, text)\n    text = re.sub(r'@\\w+', USER_TOKEN, text)\n    # Оставляем хэштеги как слова без '#'\n    text = re.sub(r'#', '', text)\n    # Нормализуем пробелы, нижний регистр\n    text = text.lower()\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ntrain_df[\"text_clean\"] = train_df[\"text\"].apply(clean_tweet)\ntest_df[\"text_clean\"]  = test_df[\"text\"].apply(clean_tweet)\n\n# =========================================================\n# 3) Сплит (если свой уже есть — можно использовать его)\n# =========================================================\nX_train, X_valid, y_train, y_valid = train_test_split(\n    train_df[\"text_clean\"], train_df[\"target\"],\n    test_size=0.15, random_state=SEED, stratify=train_df[\"target\"]\n)\n\n# =========================================================\n# 4) Токенизация под твиты (BERTweet)\n# =========================================================\nMODEL_PATH = \"/kaggle/input/bertweet/bertweet-local\"   # альтернатива: \"roberta-base\" / \"microsoft/deberta-v3-base\"\n\n# У BERTweet есть нормализатор эмодзи/твит-специфики в токенизаторе\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=True)\n\nMAX_LEN = 96  # 80–112 обычно оптимально для твитов\n\ndef tokenize_texts(texts):\n    return tokenizer(\n        list(texts),\n        padding=False,              # паддинг сделает collator\n        truncation=True,\n        max_length=MAX_LEN,\n        return_attention_mask=True\n    )\n\ntrain_enc = tokenize_texts(X_train)\nvalid_enc = tokenize_texts(X_valid)\ntest_enc  = tokenize_texts(test_df[\"text_clean\"])\n\n# =========================================================\n# 5) Torch Dataset\n# =========================================================\nclass TxtDataset(Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n    def __getitem__(self, idx):\n        item = {k: torch.tensor(v[idx]) for k,v in self.encodings.items()}\n        if self.labels is not None:\n            item[\"labels\"] = torch.tensor(int(self.labels.iloc[idx]))\n        return item\n\nds_train = TxtDataset(train_enc, y_train.reset_index(drop=True))\nds_valid = TxtDataset(valid_enc, y_valid.reset_index(drop=True))\nds_test  = TxtDataset(test_enc,  None)\n\n# =========================================================\n# 6) Модель\n# =========================================================\nnum_labels = 2\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_PATH,\n    num_labels=num_labels\n)\n\n# =========================================================\n# 7) Метрики: F1 по валидации (дальше подберём порог)\n# =========================================================\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    probs = torch.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()\n    preds = (probs >= 0.5).astype(int)  # временно 0.5\n    return {\"f1\": f1_score(labels, preds)}\n\n# =========================================================\n# 8) Тренировка\n# =========================================================\nBATCH_SIZE = 32\nEPOCHS = 4\nLR = 2e-5\n\n# Data collator сам паддит до батча\ncollator = DataCollatorWithPadding(tokenizer=tokenizer)\n\nargs = TrainingArguments(\n    output_dir=\"./checkpoints\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    num_train_epochs=EPOCHS,\n    learning_rate=LR,\n    warmup_ratio=0.06,\n    weight_decay=0.01,\n    logging_steps=50,\n    fp16=torch.cuda.is_available(),\n    report_to=[]  # отключим wandb/tensorboard по умолчанию\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=ds_train,\n    eval_dataset=ds_valid,\n    tokenizer=tokenizer,\n    data_collator=collator,\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()\n\n# =========================================================\n# 9) Подбор Порога по F1 на валидации\n# =========================================================\nvalid_logits = trainer.predict(ds_valid).predictions\nvalid_probs  = torch.softmax(torch.tensor(valid_logits), dim=1)[:, 1].numpy()\n\nbest_thr, best_f1 = 0.5, 0.0\nfor thr in np.linspace(0.1, 0.9, 161):\n    f1 = f1_score(y_valid.values, (valid_probs >= thr).astype(int))\n    if f1 > best_f1:\n        best_f1, best_thr = f1, thr\n\nprint(f\"Best F1={best_f1:.4f} @ threshold={best_thr:.3f}\")\n\nprint(classification_report(\n    y_valid.values, (valid_probs >= best_thr).astype(int), digits=4)\n)\n\n# =========================================================\n# 10) Обучение на полном train (опционально для сабмита)\n#     Склеим train+valid, дообучим 1 эпоху c маленьким LR\n# =========================================================\nFULL_FINETUNE = True\nif FULL_FINETUNE:\n    X_full = pd.concat([X_train, X_valid], ignore_index=True)\n    y_full = pd.concat([y_train, y_valid], ignore_index=True)\n\n    full_enc = tokenize_texts(X_full)\n    ds_full  = TxtDataset(full_enc, y_full)\n\n    # уменьшим LR и эпоху — модель уже натренирована\n    args_full = TrainingArguments(\n        output_dir=\"./checkpoints_full\",\n        per_device_train_batch_size=BATCH_SIZE,\n        num_train_epochs=1,\n        learning_rate=1e-5,\n        warmup_ratio=0.0,\n        weight_decay=0.01,\n        logging_steps=50,\n        fp16=torch.cuda.is_available(),\n        evaluation_strategy=\"no\",\n        save_strategy=\"no\",\n        report_to=[]\n    )\n\n    trainer_full = Trainer(\n        model=trainer.model,  # берём лучшую модель\n        args=args_full,\n        train_dataset=ds_full,\n        tokenizer=tokenizer,\n        data_collator=collator\n    )\n    trainer_full.train()\n\n# =========================================================\n# 11) Предсказания на test и формирование submission.csv\n# =========================================================\ntest_logits = trainer.model(**collator(ds_test[:])).logits.detach().cpu().numpy() \\\n    if len(ds_test) < BATCH_SIZE else trainer.predict(ds_test).predictions\ntest_probs  = torch.softmax(torch.tensor(test_logits), dim=1)[:, 1].numpy()\ntest_preds  = (test_probs >= best_thr).astype(int)\n\nsubmission = pd.DataFrame({\n    \"id\": test_df[\"id\"],\n    \"target\": test_preds\n})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Saved submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T20:22:28.574048Z","iopub.execute_input":"2025-10-07T20:22:28.574773Z","iopub.status.idle":"2025-10-07T20:22:32.016996Z","shell.execute_reply.started":"2025-10-07T20:22:28.574743Z","shell.execute_reply":"2025-10-07T20:22:32.015699Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_741/841732712.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0mcollator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataCollatorWithPadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m args = TrainingArguments(\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./checkpoints\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mevaluation_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"],"ename":"TypeError","evalue":"TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"!pip install -U transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}